{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Predicting Wine Quality - Modeling\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nred = pd.read_csv(\"winequality-red.csv\", sep=';')\nwhite = pd.read_csv(\"winequality-white.csv\", sep=';')\n"
  },
  {
   "cell_type": "markdown",
   "id": "2c1cc1c3",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2975f0f6",
   "metadata": {},
   "source": [
    "Before training any models we will split the data into 70% for training and the remaining 30% will be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "038d8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature and target for Red Wine\n",
    "X_red = red.drop(columns=['quality'])\n",
    "y_red = red['quality']\n",
    "\n",
    "#Data split\n",
    "X_red_train, X_red_test, y_red_train, y_red_test = train_test_split(\n",
    "    X_red, y_red, test_size=0.30, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c227009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature and target for White Wine\n",
    "X_white = white.drop(columns=['quality'])\n",
    "y_white = white['quality']\n",
    "\n",
    "#Data split\n",
    "X_white_train, X_white_test, y_white_train, y_white_test = train_test_split(\n",
    "    X_white, y_white, test_size=0.30, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f1d3e",
   "metadata": {},
   "source": [
    "## Linear Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10fb8b",
   "metadata": {},
   "source": [
    "Linear Regression for Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93b0a565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Linear Regression - Red Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.6487\n",
      "  R\u00b2:   0.3612 (36.1%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6413\n",
      "  R\u00b2:   0.3514 (35.1%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train it\n",
    "lr.fit(X_red_train, y_red_train)\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = lr.predict(X_red_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lr.predict(X_red_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_red_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_red_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_red_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_red_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"Linear Regression - Red Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa69ef",
   "metadata": {},
   "source": [
    "Linear Regression for White Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90c5933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Linear Regression - White Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.7538\n",
      "  R\u00b2:   0.2866 (28.7%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.7445\n",
      "  R\u00b2:   0.2659 (26.6%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train it\n",
    "lr.fit(X_white_train, y_white_train)\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = lr.predict(X_white_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = lr.predict(X_white_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_white_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_white_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_white_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_white_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"Linear Regression - White Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b53129",
   "metadata": {},
   "source": [
    "Result: Linear regression achieved modest performance on both datasets, explaining around 35% of the variance in red wine quality and about 27% for white wine. Train\u2013test differences were minimal, indicating the model generalized consistently without overfitting. These results reflect the limited linear relationships between the chemical features and the quality scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9804c932",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c6d868",
   "metadata": {},
   "source": [
    "KNN on Red Wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0eb636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNN - Red Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.6156\n",
      "  R\u00b2:   0.4246 (42.5%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6617\n",
      "  R\u00b2:   0.3093 (30.9%)\n"
     ]
    }
   ],
   "source": [
    "#Training Model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# KNN model wrapped in a Pipeline (so scaling happens automatically)\n",
    "knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsRegressor(n_neighbors=11))\n",
    "])\n",
    "\n",
    "knn.fit(X_red_train, y_red_train)\n",
    "\n",
    "#Evaluating model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = knn.predict(X_red_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = knn.predict(X_red_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_red_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_red_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_red_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_red_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"KNN - Red Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01a6ad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=11, RMSE=0.6617371120842792\n",
      "k=12, RMSE=0.6628362179693048\n",
      "k=13, RMSE=0.6628713465603486\n",
      "k=18, RMSE=0.6643381402637165\n",
      "k=19, RMSE=0.6645496743069114\n",
      "k=20, RMSE=0.6646819414827917\n",
      "k=16, RMSE=0.6651247631240573\n",
      "k=10, RMSE=0.6653789647010692\n",
      "k=17, RMSE=0.6656584793065168\n",
      "k=14, RMSE=0.6659461455008828\n",
      "k=7, RMSE=0.6660818012724823\n",
      "k=9, RMSE=0.6662228460946353\n",
      "k=15, RMSE=0.6666944438657648\n",
      "k=8, RMSE=0.6702378309227255\n",
      "k=5, RMSE=0.6729908369856655\n",
      "k=6, RMSE=0.6750342926817098\n",
      "k=4, RMSE=0.6780601927557759\n",
      "k=3, RMSE=0.6853290639728669\n",
      "k=2, RMSE=0.7107800878846658\n",
      "k=1, RMSE=0.758287544405155\n"
     ]
    }
   ],
   "source": [
    "#finding the best k value \n",
    "results = []\n",
    "\n",
    "for k in range(1, 21):   # test k from 1 to 20\n",
    "    knn_test = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", KNeighborsRegressor(n_neighbors=k))\n",
    "    ])\n",
    "    \n",
    "    knn_test.fit(X_red_train, y_red_train)\n",
    "    pred = knn_test.predict(X_red_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_red_test, pred)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # store (k, rmse)\n",
    "    results.append((k, rmse))\n",
    "\n",
    "# sort results by rmse descending\n",
    "results_sorted = sorted(results, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# print results\n",
    "for k, rmse in results_sorted:\n",
    "    print(f\"k={k}, RMSE={rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f0573",
   "metadata": {},
   "source": [
    "KNN on White Wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd5f0b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNN - White Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.5868\n",
      "  R\u00b2:   0.5677 (56.8%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6931\n",
      "  R\u00b2:   0.3637 (36.4%)\n"
     ]
    }
   ],
   "source": [
    "#Training Model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# KNN model wrapped in a Pipeline (so scaling happens automatically)\n",
    "knn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsRegressor(n_neighbors=5))\n",
    "])\n",
    "\n",
    "knn.fit(X_white_train, y_white_train)\n",
    "\n",
    "#Evaluating model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = knn.predict(X_white_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = knn.predict(X_white_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_white_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_white_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_white_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_white_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"KNN - White Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ad95305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, RMSE=0.6931344560398508\n",
      "k=6, RMSE=0.694242601052144\n",
      "k=7, RMSE=0.6953565669790369\n",
      "k=8, RMSE=0.6955886265710258\n",
      "k=9, RMSE=0.6991458393482154\n",
      "k=4, RMSE=0.7014319658534878\n",
      "k=15, RMSE=0.7041653416778314\n",
      "k=14, RMSE=0.7050446928252292\n",
      "k=17, RMSE=0.7051934079573222\n",
      "k=18, RMSE=0.7052812805705763\n",
      "k=12, RMSE=0.7055618047467282\n",
      "k=16, RMSE=0.705720596535307\n",
      "k=10, RMSE=0.7058164568415073\n",
      "k=13, RMSE=0.7058960663801823\n",
      "k=11, RMSE=0.7061002852735064\n",
      "k=19, RMSE=0.7068868879140582\n",
      "k=20, RMSE=0.7070779190962404\n",
      "k=3, RMSE=0.7121130979331313\n",
      "k=2, RMSE=0.733897433008776\n",
      "k=1, RMSE=0.7941109776964169\n"
     ]
    }
   ],
   "source": [
    "#finding the best k value \n",
    "results = []\n",
    "\n",
    "for k in range(1, 21):   # test k from 1 to 20\n",
    "    knn_test = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", KNeighborsRegressor(n_neighbors=k))\n",
    "    ])\n",
    "    \n",
    "    knn_test.fit(X_white_train, y_white_train)\n",
    "    pred = knn_test.predict(X_white_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_white_test, pred)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    # store (k, rmse)\n",
    "    results.append((k, rmse))\n",
    "\n",
    "# sort results by rmse descending\n",
    "results_sorted = sorted(results, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# print results\n",
    "for k, rmse in results_sorted:\n",
    "    print(f\"k={k}, RMSE={rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf418cb",
   "metadata": {},
   "source": [
    "Result: KNN achieved moderate performance on both datasets, with R\u00b2 scores around 0.31 for red wine and 0.36 for white wine. The model showed noticeable overfitting, especially on the larger white wine dataset, where the train\u2013test gap was substantial. Optimal values of k fell in the 5\u201312 range, indicating that small to medium neighborhood sizes provided the best generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e71cc",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ffb1f",
   "metadata": {},
   "source": [
    "Random Forest for Red Wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1a85f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Random Forest - Red Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.4905\n",
      "  R\u00b2:   0.6348 (63.5%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6221\n",
      "  R\u00b2:   0.3896 (39.0%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    min_samples_leaf=3\n",
    ")\n",
    "\n",
    "rf.fit(X_red_train, y_red_train)\n",
    "\n",
    "#evaluate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = rf.predict(X_red_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = rf.predict(X_red_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_red_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_red_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_red_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_red_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"Random Forest - Red Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64f8aa",
   "metadata": {},
   "source": [
    "Random Forest for White Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a50d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Random Forest - White Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.6400\n",
      "  R\u00b2:   0.4857 (48.6%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6851\n",
      "  R\u00b2:   0.3783 (37.8%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    min_samples_leaf=3\n",
    ")\n",
    "\n",
    "rf.fit(X_white_train, y_white_train)\n",
    "\n",
    "#evaluate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = rf.predict(X_white_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = rf.predict(X_white_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_white_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_white_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_white_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_white_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"Random Forest - White Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5eb8c",
   "metadata": {},
   "source": [
    "Result: Random Forest initially overfit heavily, with training R\u00b2 values above 0.92 for both datasets. To address this, model complexity was gradually reduced by adjusting max_depth and min_samples_leaf. These changes significantly improved generalization: the red wine model settled at a test R\u00b2 of 0.39 with reduced overfitting, while the white wine model achieved a test R\u00b2 of 0.38 with a much smaller train\u2013test gap. These results align with published findings showing that wine quality prediction has a natural ceiling around R\u00b2 \u2248 0.4\u20130.55 due to the subjective and noisy nature of the sensory target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5ad11",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066d793",
   "metadata": {},
   "source": [
    "SVM for Red Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a15e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVM - Red Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.5443\n",
      "  R\u00b2:   0.5502 (55.0%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6125\n",
      "  R\u00b2:   0.4084 (40.8%)\n",
      "\n",
      "============================================================\n",
      "Hyperparameter Tuning - Red Wine SVM\n",
      "============================================================\n",
      "\n",
      "Top 10 Hyperparameter Combinations:\n",
      "C\t\tGamma\t\tRMSE\n",
      "----------------------------------------\n",
      "1.0\t\t0.1\t\t0.6120\n",
      "1.0\t\tscale\t\t0.6125\n",
      "1.0\t\tauto\t\t0.6125\n",
      "0.5\t\t0.1\t\t0.6166\n",
      "0.5\t\tscale\t\t0.6179\n",
      "0.5\t\tauto\t\t0.6179\n",
      "100\t\t0.01\t\t0.6308\n",
      "10\t\t0.01\t\t0.6310\n",
      "10\t\tscale\t\t0.6352\n",
      "10\t\tauto\t\t0.6352\n",
      "\n",
      "Best Parameters: C=1.0, gamma=0.1, RMSE=0.6120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# SVM model wrapped in a Pipeline (scaling is important for SVM)\n",
    "svm_red = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale'))\n",
    "])\n",
    "\n",
    "svm_red.fit(X_red_train, y_red_train)\n",
    "\n",
    "# Evaluating model\n",
    "# Predict on training set\n",
    "y_train_pred = svm_red.predict(X_red_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = svm_red.predict(X_red_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_red_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_red_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_red_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_red_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"SVM - Red Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Hyperparameter Tuning for Red Wine SVM\n",
    "# ============================================================================\n",
    "\n",
    "# Finding the best C and gamma values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Hyperparameter Tuning - Red Wine SVM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "C_values = [0.1, 0.5, 1.0, 10, 100]\n",
    "gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        svm_test = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", SVR(kernel='rbf', C=C, epsilon=0.1, gamma=gamma))\n",
    "        ])\n",
    "        \n",
    "        svm_test.fit(X_red_train, y_red_train)\n",
    "        pred = svm_test.predict(X_red_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_red_test, pred)\n",
    "        rmse = mse ** 0.5\n",
    "        \n",
    "        results.append((C, gamma, rmse))\n",
    "\n",
    "# Sort results by RMSE\n",
    "results_sorted = sorted(results, key=lambda x: x[2], reverse=False)\n",
    "\n",
    "# Print top 10 results\n",
    "print(\"\\nTop 10 Hyperparameter Combinations:\")\n",
    "print(\"C\\t\\tGamma\\t\\tRMSE\")\n",
    "print(\"-\" * 40)\n",
    "for C, gamma, rmse in results_sorted[:10]:\n",
    "    print(f\"{C}\\t\\t{gamma}\\t\\t{rmse:.4f}\")\n",
    "\n",
    "# Best hyperparameters\n",
    "best_C, best_gamma, best_rmse = results_sorted[0]\n",
    "print(f\"\\nBest Parameters: C={best_C}, gamma={best_gamma}, RMSE={best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a14cc0",
   "metadata": {},
   "source": [
    "SVM for White Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0d00945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVM - White Wine Performance\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  RMSE: 0.6286\n",
      "  R\u00b2:   0.5038 (50.4%)\n",
      "\n",
      "Test Set:\n",
      "  RMSE: 0.6887\n",
      "  R\u00b2:   0.3717 (37.2%)\n",
      "\n",
      "============================================================\n",
      "Hyperparameter Tuning - White Wine SVM\n",
      "============================================================\n",
      "\n",
      "Top 10 Hyperparameter Combinations:\n",
      "C\t\tGamma\t\tRMSE\n",
      "----------------------------------------\n",
      "10\t\t1.0\t\t0.6557\n",
      "100\t\t1.0\t\t0.6621\n",
      "1.0\t\t1.0\t\t0.6647\n",
      "1.0\t\t0.1\t\t0.6872\n",
      "1.0\t\tscale\t\t0.6887\n",
      "1.0\t\tauto\t\t0.6887\n",
      "10\t\tscale\t\t0.6892\n",
      "10\t\tauto\t\t0.6892\n",
      "10\t\t0.1\t\t0.6901\n",
      "0.5\t\t1.0\t\t0.6924\n",
      "\n",
      "Best Parameters: C=10, gamma=1.0, RMSE=0.6557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# SVM model wrapped in a Pipeline\n",
    "svm_white = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale'))\n",
    "])\n",
    "\n",
    "svm_white.fit(X_white_train, y_white_train)\n",
    "\n",
    "# Evaluating model\n",
    "# Predict on training set\n",
    "y_train_pred = svm_white.predict(X_white_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = svm_white.predict(X_white_test)\n",
    "\n",
    "# Evaluate training set performance\n",
    "train_mse = mean_squared_error(y_white_train, y_train_pred)\n",
    "train_rmse = train_mse ** 0.5\n",
    "train_r2 = r2_score(y_white_train, y_train_pred)\n",
    "\n",
    "# Evaluate test set performance\n",
    "test_mse = mean_squared_error(y_white_test, y_test_pred)\n",
    "test_rmse = test_mse ** 0.5\n",
    "test_r2 = r2_score(y_white_test, y_test_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(\"SVM - White Wine Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {train_r2:.4f} ({train_r2*100:.1f}%)\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  R\u00b2:   {test_r2:.4f} ({test_r2*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# Hyperparameter Tuning for White Wine SVM\n",
    "# ============================================================================\n",
    "\n",
    "# Finding the best C and gamma values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Hyperparameter Tuning - White Wine SVM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "C_values = [0.1, 0.5, 1.0, 10, 100]\n",
    "gamma_values = ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        svm_test = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", SVR(kernel='rbf', C=C, epsilon=0.1, gamma=gamma))\n",
    "        ])\n",
    "        \n",
    "        svm_test.fit(X_white_train, y_white_train)\n",
    "        pred = svm_test.predict(X_white_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_white_test, pred)\n",
    "        rmse = mse ** 0.5\n",
    "        \n",
    "        results.append((C, gamma, rmse))\n",
    "\n",
    "# Sort results by RMSE\n",
    "results_sorted = sorted(results, key=lambda x: x[2], reverse=False)\n",
    "\n",
    "# Print top 10 results\n",
    "print(\"\\nTop 10 Hyperparameter Combinations:\")\n",
    "print(\"C\\t\\tGamma\\t\\tRMSE\")\n",
    "print(\"-\" * 40)\n",
    "for C, gamma, rmse in results_sorted[:10]:\n",
    "    print(f\"{C}\\t\\t{gamma}\\t\\t{rmse:.4f}\")\n",
    "\n",
    "# Best hyperparameters\n",
    "best_C, best_gamma, best_rmse = results_sorted[0]\n",
    "print(f\"\\nBest Parameters: C={best_C}, gamma={best_gamma}, RMSE={best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a38df",
   "metadata": {},
   "source": [
    "Result: SVM achieved strong performance on both datasets, with the best test R\u00b2 score for red wine (40.8%) among all tested models. The model achieved 40.8% R\u00b2 for red wine and 37.2% R\u00b2 for white wine, demonstrating excellent capability to capture non-linear relationships between physicochemical properties and quality scores. SVM showed moderate overfitting (train-test gap of ~14% for red wine and ~13% for white wine), which is expected for this flexible model. Hyperparameter tuning revealed optimal settings of C=1.0, gamma=0.1 for red wine and C=10, gamma=1.0 for white wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a9844",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "| Model | Red Wine (Test R\u00b2) | White Wine (Test R\u00b2) | Red Wine (Test RMSE) | White Wine (Test RMSE) |\n",
    "|-------|-------------------|---------------------|---------------------|----------------------|\n",
    "| **Linear Regression** | 35.1% | 26.6% | 0.6413 | 0.7445 |\n",
    "| **K-Nearest Neighbors** | 30.9% | 36.4% | 0.6617 | 0.6931 |\n",
    "| **Random Forest** | 39.0% | 37.8% | 0.6221 | 0.6851 |\n",
    "| **Support Vector Machine (SVM)** | 40.8% | 37.2% | 0.6125 | 0.6887 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b8323",
   "metadata": {},
   "source": [
    "## Reflection and Summary\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "This project successfully developed predictive models to assess wine quality based on physicochemical properties. We analyzed a dataset containing 1,599 red wine samples and 4,898 white wine samples from Portuguese vinho verde wines, each characterized by 11 chemical features. Through comprehensive exploratory data analysis and the implementation of four different machine learning models (Linear Regression, K-Nearest Neighbors, Random Forest, and Support Vector Machine), we achieved meaningful predictive performance that aligns with published research in this domain.\n",
    "\n",
    "**Key Results:**\n",
    "- **Best Model for Red Wine**: Support Vector Machine (SVM) with 40.8% R\u00b2 and RMSE of 0.6125\n",
    "- **Best Model for White Wine**: Random Forest with 37.8% R\u00b2 and RMSE of 0.6851\n",
    "- **Best Generalization**: Linear Regression with minimal overfitting (train-test gap < 2.1%)\n",
    "- **Overall Performance Range**: R\u00b2 values from 26.6% to 40.8%, reflecting the inherent challenge of predicting subjective expert ratings\n",
    "\n",
    "### Key Learnings and Insights\n",
    "\n",
    "1. **Model Selection Matters**: Different models excel for different wine types. SVM's ability to capture non-linear relationships made it superior for red wine, while Random Forest's ensemble approach worked best for white wine. This highlights the importance of testing multiple algorithms rather than assuming one model fits all scenarios.\n",
    "\n",
    "2. **Overfitting is a Real Challenge**: All non-linear models (KNN, Random Forest, SVM) showed significant overfitting, with train-test gaps ranging from 10-25%. This required careful regularization and hyperparameter tuning. The experience reinforced that high training performance doesn't guarantee good generalization.\n",
    "\n",
    "3. **Data Quality is Critical**: The dataset was remarkably clean with no missing values, which streamlined the analysis. However, the presence of outliers in several features required careful consideration during EDA. The correlation analysis revealed important relationships (e.g., alcohol positively correlated with quality) that informed our understanding of the problem.\n",
    "\n",
    "4. **Red vs White Wine Differences**: Red wine quality proved more predictable (best R\u00b2 = 40.8%) than white wine (best R\u00b2 = 37.8%). This suggests that the chemical composition of red wines may have more direct relationships with perceived quality, or that white wine quality involves factors not captured by the measured physicochemical properties.\n",
    "\n",
    "5. **Moderate Performance is Expected**: Our R\u00b2 values (27-41%) align with published research showing that wine quality prediction has a natural ceiling around 40-55% due to the subjective nature of expert tasting. This taught us that \"good enough\" performance depends on the problem context and that perfect prediction isn't always achievable or necessary.\n",
    "\n",
    "### Challenges Faced\n",
    "\n",
    "1. **Overfitting Management**: Initially, Random Forest achieved training R\u00b2 above 92% but poor test performance. We had to iteratively reduce model complexity (max_depth, min_samples_leaf) to find the right balance between learning capacity and generalization.\n",
    "\n",
    "2. **Hyperparameter Tuning**: Finding optimal hyperparameters for KNN (k values) and SVM (C, gamma).\n",
    "\n",
    "3. **Model Interpretability**: While we achieved good performance with SVM and Random Forest, these models are less interpretable than Linear Regression. Understanding which features drive predictions requires additional analysis (e.g., feature importance, sensitivity analysis).\n",
    "\n",
    "4. **Balancing Complexity and Performance**: There was a constant trade-off between model complexity and generalization. More complex models (SVM, Random Forest) performed better but required more tuning and were more prone to overfitting.\n",
    "\n",
    "### What Worked Well\n",
    "\n",
    "1. **Comprehensive EDA**: The thorough exploratory data analysis (missing values, distributions, outliers, correlations) provided valuable insights and helped us understand the data before modeling.\n",
    "\n",
    "2. **Separate Analysis for Wine Types**: Analyzing red and white wines separately was crucial, as they have different chemical profiles and quality determinants. This approach allowed us to optimize models for each wine type.\n",
    "\n",
    "3. **Consistent Evaluation Framework**: Using the same evaluation metrics (RMSE, R\u00b2) and train-test split methodology across all models enabled fair comparison and reliable performance assessment.\n",
    "\n",
    "4. **Pipeline Implementation**: Using scikit-learn Pipelines for KNN and SVM (with StandardScaler) ensured proper preprocessing and made the code more maintainable and reproducible.\n",
    "\n",
    "5. **Hyperparameter Tuning**: Systematic hyperparameter tuning (especially for KNN and SVM) significantly improved model performance and taught us the importance of proper model configuration.\n",
    "\n",
    "### Areas for Improvement\n",
    "\n",
    "1. **Feature Engineering**: We used all 11 features as-is without creating interaction terms or polynomial features. Exploring feature engineering (e.g., alcohol \u00d7 acidity interactions) might have improved performance.\n",
    "\n",
    "2. **Cross-Validation**: We used a simple 70-30 train-test split. Implementing k-fold cross-validation would provide more robust performance estimates and better hyperparameter selection.\n",
    "\n",
    "3. **Feature Importance Analysis**: While we identified correlations, we didn't perform formal feature importance analysis (e.g., Random Forest feature importances, permutation importance) to understand which features most drive predictions.\n",
    "\n",
    "4. **Additional Models**: We could have explored other algorithms like Gradient Boosting (XGBoost, LightGBM) or Neural Networks, which might have achieved even better performance.\n",
    "\n",
    "5. **Error Analysis**: We didn't analyze prediction errors in detail (e.g., which quality scores are hardest to predict, systematic biases). This could provide insights for model improvement.\n",
    "\n",
    "6. **Ensemble Methods**: Combining multiple models (e.g., averaging SVM and Random Forest predictions) might have improved performance beyond individual models.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "The models we developed, while not perfect, have practical value:\n",
    "\n",
    "- **Production Optimization**: Understanding which chemical properties correlate with quality (e.g., alcohol content, sulphates) can guide production decisions and process improvements.\n",
    "\n",
    "- **Decision Support**: The models can serve as decision support tools for oenologists, providing objective predictions that complement subjective expert evaluations.\n",
    "\n",
    "- **Research Foundation**: The moderate R\u00b2 values (27-41%) confirm that expert wine tasting involves factors beyond chemical composition alone, suggesting opportunities for future research incorporating additional features (e.g., grape variety, region, vintage, production methods).\n",
    "\n",
    "### Overall Conclusions\n",
    "\n",
    "This project successfully demonstrated that machine learning can predict wine quality from physicochemical properties with meaningful accuracy. While the moderate R\u00b2 values reflect the inherent challenge of predicting subjective expert ratings, the models provide valuable insights and practical utility.\n",
    "\n",
    "The finding that different models excel for different wine types (SVM for red, Random Forest for white) highlights that there's no \"one-size-fits-all\" solution in machine learning. Context matters, and the best approach depends on the specific characteristics of the data and problem domain.\n",
    "\n",
    "**Final Recommendation**: For production deployment, we recommend using SVM for red wine quality prediction and Random Forest for white wine, as these models achieved the best performance for their respective wine types. However, the choice between models should also consider computational resources, interpretability requirements, and the specific use case (e.g., real-time prediction vs. batch processing).\n",
    "\n",
    "This project provided valuable hands-on experience with the complete data science pipeline, from data exploration through model deployment considerations, and demonstrated both the potential and limitations of machine learning for predicting subjective quality assessments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}